# Experiments

This folder contains the scripts to run vector search and RALM on CPUs and GPUs. 


## Vector Search

Eventually we determine to use 1000M, because RALM-L2000M using 4 FPGAs seem always to have some problems (stuck in the middle during query time).

### CPU & GPU baselines

All experiments can be executed using the following single command. 

```
vector_search_baseline.py
```

Evaluate CPU-only and CPU-GPU vector search, using local Faiss (without considering network latency).

Each query batch runs for 100 times (with some warm-up queries), and the latency distribution is recorded. 

### FPGA vector search.

hacc-build IP: 10.1.212.110

#### SIFT1000M 

Takes around 45 min to finish SIFT1000M.

Run the following 3 commands, loop over (1) batch size from 1 to 128, (2) architecture in "1FPGA-8CPU" and "1FPGA-1GPU". That is 9 x 2 = 18 sets of experiments in total. `xbutil reset --device` needed after each FPGA run.

```
# term 1 (hacc-build): in FPGA-PQ-disaggregated-memory/CPU_programs : launch coordinator, make sure the config/xxx.yaml is set correctly
python launch_CPU_and_FPGA.py --config_fname ./config/SIFT1000M_M16.yaml --mode CPU_coordinator --TOPK 100 --batch_size 1 --total_batch_num 105

# term 2 (FPGA): the folloiwng command is generated by running the cooridnator launcher
./host/host build_dir.hw.xilinx_u250_gen3x16_xdma_4_1_202210_1/network.xclbin 10.253.74.12 8881 10.1.212.110 5001 1 

# term 3 (GPU server) vecDB, change the architecture to "1FPGA-1GPU" for FPGA-GPU experiments
python vector_search_FPGA.py --search_base_config config/search_SIFT1000M.yaml         --search_server_host "10.1.212.110" --search_server_port 9091         --batch_size 1 --n_warmup_batch 5 --n_batch 100 --architecture "1FPGA-8CPU" --use_gpu_id 0 --mode "latency" --load_dict 1
```

For throughput measurement, change the last command to the following, and set the batch size of the first command to 128:

```
python vector_search_FPGA.py --search_base_config config/search_SIFT1000M.yaml \
        --search_server_host "10.1.212.110" --search_server_port 9091 \
        --batch_size 128 --n_warmup_batch 5 --n_batch 20 --architecture "1FPGA-8CPU" --use_gpu_id 0 --mode "throughput" --throughput_runs 5 --load_dict 1
```

### Deep1000M

Takes around 40 min to finish Deep1000M.

Run the following 3 commands, loop over (1) batch size from 1 to 128, (2) architecture in "1FPGA-8CPU" and "1FPGA-1GPU". That is 9 x 2 = 18 sets of experiments in total. `xbutil reset --device` needed after each FPGA run.

```
# term 1 (hacc-build): launch coordinator
python launch_CPU_and_FPGA.py --config_fname ./config/Deep1000M_M16.yaml --mode CPU_coordinator --TOPK 100 --batch_size 1 --total_batch_num 105

# term 2 (FPGA): the folloiwng command is generated by running the cooridnator launcher
./host/host build_dir.hw.xilinx_u250_gen3x16_xdma_4_1_202210_1/network.xclbin 10.253.74.16 8882 10.1.212.110 5002 2 

# term 3 (GPU server) vecDB, change the architecture to "1FPGA-1GPU" for FPGA-GPU experiments
python vector_search_FPGA.py --search_base_config config/search_Deep1000M.yaml         --search_server_host "10.1.212.110" --search_server_port 9091         --batch_size 1 --n_warmup_batch 5 --n_batch 100 --architecture "1FPGA-8CPU" --use_gpu_id 0 --mode "latency" --load_dict 1
```

For throughput measurement, change the last command to the following, and set the batch size of the first command to 128:

```
python vector_search_FPGA.py --search_base_config config/search_Deep1000M.yaml \
        --search_server_host "10.1.212.110" --search_server_port 9091 \
        --batch_size 128 --n_warmup_batch 5 --n_batch 20 --architecture "1FPGA-8CPU" --use_gpu_id 0 --mode "throughput" --throughput_runs 5 --load_dict 1
```

### RALM-S1000M 

Follow the similar steps. 

### RALM-L1000M & RALM-S2000M 

Takes around 60 min to finish RALM-L1000M.

Run the following 4 commands, loop over (1) batch size from 1 to 128, (2) architecture in "2FPGA-8CPU" and "2FPGA-1GPU". That is 9 x 2 = 18 sets of experiments in total. `xbutil reset --device` needed after each FPGA run.

Note: FPGA 1 has to be a few seconds (5?) later than FPGA 0 (must start in order).

```
# term 1 (hacc-build): launch coordinator
python launch_CPU_and_FPGA.py --config_fname ./config/RALM-L1000M_M64.yaml --mode CPU_coordinator --TOPK 100 --batch_size 1 --total_batch_num 105

# term 2 (FPGA 0): the folloiwng command is generated by running the cooridnator launcher
./host/host build_dir.hw.xilinx_u250_gen3x16_xdma_4_1_202210_1/network.xclbin 10.253.74.16 8882 10.1.212.110 5002 2  

# term 3 (FPGA 1): the folloiwng command is generated by running the cooridnator launcher
./host/host build_dir.hw.xilinx_u250_gen3x16_xdma_4_1_202210_1/network.xclbin 10.253.74.20 8883 10.1.212.110 5003 3

# term 4 (GPU server) vecDB
python vector_search_FPGA.py --search_base_config config/search_RALM-L1000M.yaml         --search_server_host "10.1.212.110" --search_server_port 9091         --batch_size 1 --n_warmup_batch 5 --n_batch 100 --architecture "2FPGA-8CPU" --use_gpu_id 0 --mode "latency" --load_dict 1
```

### RALM-L2000M

Run the following 4 commands, loop over (1) batch size from 1 to 128, (2) architecture in "4FPGA-8CPU" and "4FPGA-1GPU". That is 9 x 2 = 18 sets of experiments in total. `xbutil reset --device` needed after each FPGA run.

Note: FPGA 3 has to be a few seconds (5?) later than FPGA 2 (must start in order) > FPGA 1 > FPGA 0.

(pipeline can stuck when batch size = 128/64 -> TODO: try other ports, etc.)

```
# term 1 (hacc-build): launch coordinator
python launch_CPU_and_FPGA.py --config_fname ./config/RALM-L2000M_M64.yaml --mode CPU_coordinator --TOPK 100 --batch_size 1 --total_batch_num 105

# term 2 (FPGA 0): the folloiwng command is generated by running the cooridnator launcher
./host/host build_dir.hw.xilinx_u250_gen3x16_xdma_4_1_202210_1/network.xclbin 10.253.74.16 8882 10.1.212.110 5002 2  

# term 3 (FPGA 1): the folloiwng command is generated by running the cooridnator launcher
./host/host build_dir.hw.xilinx_u250_gen3x16_xdma_4_1_202210_1/network.xclbin 10.253.74.20 8883 10.1.212.110 5003 3

# term 4 (FPGA 2): the folloiwng command is generated by running the cooridnator launcher
./host/host build_dir.hw.xilinx_u250_gen3x16_xdma_4_1_202210_1/network.xclbin 10.253.74.24 8884 10.1.212.110 5004 4  

# term 5 (FPGA 3): the folloiwng command is generated by running the cooridnator launcher
./host/host build_dir.hw.xilinx_u250_gen3x16_xdma_4_1_202210_1/network.xclbin 10.253.74.28 8885 10.1.212.110 5005 5  

# term 4 (GPU server) vecDB
python vector_search_FPGA.py --search_base_config config/search_RALM-L2000M.yaml         --search_server_host "10.1.212.110" --search_server_port 9091         --batch_size 1 --n_warmup_batch 5 --n_batch 100 --architecture "4FPGA-8CPU" --use_gpu_id 0 --mode "latency" --load_dict 1
```



## RALM

Baseline: GPU inference + CPU-only vector search (given that we proved in the baseline that CPU + GPU is not much faster)

Ours: GPU inference + FPGA-GPU vector search (given in the baseline we proved that GPU-FPGA is faster than CPU-FPGA).

To run the experiments, use `python print_server_coordinator_gpu_cmd.py --model_base_config config/...yaml --coordinator_base_config config/...yaml --runtime_config config/...yaml` with arguments, and follow the instructions (one for starting the GPU side, one for starting the search server side)

Per experiment, set (a) --model_base_config config/Dec-S.yaml (b) ralm_runtime.yaml, which mainly includes (server settings, batch_size, retrieval_interval, architecture, request_with_lists).

### Dec-S 

Latency: 

```
# CPU
python print_server_coordinator_gpu_cmd.py --model_base_config config/Dec-S.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_Dec-S_CPU_latency.yaml

# FPGA
python print_server_coordinator_gpu_cmd.py --model_base_config config/Dec-S.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_Dec-S_FPGA_latency.yaml
```

Throughput: change `batch_size` to 64:

```
# CPU
python print_server_coordinator_gpu_cmd.py --model_base_config config/Dec-S.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_Dec-S_CPU_throughput.yaml

# FPGA
python print_server_coordinator_gpu_cmd.py --model_base_config config/Dec-S.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_Dec-S_FPGA_throughput.yaml
```

For breaking down the latency / throughput of only inference (dummy retrieval), use the `coord_gpu_cmd` printed, but use a separate random answer server

```
# Latency: use the `dummy_search_cmd` and `coord_gpu_cmd` printed
python print_server_coordinator_gpu_cmd.py --model_base_config config/Dec-S.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_Dec-S_latency_only_inference.yaml

# Throughput: use the `dummy_search_cmd` and `coord_gpu_cmd` printed
python print_server_coordinator_gpu_cmd.py --model_base_config config/Dec-S.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_Dec-S_throughput_only_inference.yaml
```

### EncDec-S

PLEASE EDIT BEFORE RUN: retrieval_interval (EncDec): 8 / 64 / 512

Loop over the three retrieval intervals.

Latency: 

```
# CPU
python print_server_coordinator_gpu_cmd.py --model_base_config config/EncDec-S.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_EncDec-S_CPU_latency.yaml

# FPGA
python print_server_coordinator_gpu_cmd.py --model_base_config config/EncDec-S.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_EncDec-S_FPGA_latency.yaml
```

Throughput: change `batch_size` to 64:

```
# CPU
python print_server_coordinator_gpu_cmd.py --model_base_config config/EncDec-S.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_EncDec-S_CPU_throughput.yaml

# FPGA
python print_server_coordinator_gpu_cmd.py --model_base_config config/EncDec-S.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_EncDec-S_FPGA_throughput.yaml
```

For breaking down the latency / throughput of only inference (dummy retrieval), use the `coord_gpu_cmd` printed, but use a separate random answer server

```
# Latency: use the `dummy_search_cmd` and `coord_gpu_cmd` printed
# PLEASE EDIT BEFORE RUN: retrieval_interval (EncDec): 8 / 64 / 512
python print_server_coordinator_gpu_cmd.py --model_base_config config/EncDec-S.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_EncDec-S_latency_only_inference.yaml

# Throughput: use the `dummy_search_cmd` and `coord_gpu_cmd` printed
# PLEASE EDIT BEFORE RUN: retrieval_interval (EncDec): 8 / 64 / 512
python print_server_coordinator_gpu_cmd.py --model_base_config config/EncDec-S.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_EncDec-S_throughput_only_inference.yaml
```

### Dec-L

Latency: 

```
# CPU
python print_server_coordinator_gpu_cmd.py --model_base_config config/Dec-L.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_Dec-L_CPU_latency.yaml

# FPGA
python print_server_coordinator_gpu_cmd.py --model_base_config config/Dec-L.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_Dec-L_FPGA_latency.yaml
```

Throughput: change `batch_size` to 8:

```
# CPU
python print_server_coordinator_gpu_cmd.py --model_base_config config/Dec-L.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_Dec-L_CPU_throughput.yaml

# FPGA
python print_server_coordinator_gpu_cmd.py --model_base_config config/Dec-L.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_Dec-L_FPGA_throughput.yaml
```

For breaking down the latency / throughput of only inference (dummy retrieval), use the `coord_gpu_cmd` printed, but use a separate random answer server

```
# Latency: use the `dummy_search_cmd` and `coord_gpu_cmd` printed
python print_server_coordinator_gpu_cmd.py --model_base_config config/Dec-L.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_Dec-L_latency_only_inference.yaml

# Throughput: use the `dummy_search_cmd` and `coord_gpu_cmd` printed
python print_server_coordinator_gpu_cmd.py --model_base_config config/Dec-L.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_Dec-L_throughput_only_inference.yaml
```

### EncDec-L

PLEASE EDIT BEFORE RUN: retrieval_interval (EncDec): 8 / 64 / 512

Loop over the three retrieval intervals.

Latency: 

```
# CPU
python print_server_coordinator_gpu_cmd.py --model_base_config config/EncDec-L.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_EncDec-L_CPU_latency.yaml

# FPGA
python print_server_coordinator_gpu_cmd.py --model_base_config config/EncDec-L.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_EncDec-L_FPGA_latency.yaml
```

Throughput: change `batch_size` to 8:

```
# CPU
python print_server_coordinator_gpu_cmd.py --model_base_config config/EncDec-L.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_EncDec-L_CPU_throughput.yaml

# FPGA
python print_server_coordinator_gpu_cmd.py --model_base_config config/EncDec-L.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_EncDec-L_FPGA_throughput.yaml
```

For breaking down the latency / throughput of only inference (dummy retrieval), use the `coord_gpu_cmd` printed, but use a separate random answer server

```
# Latency: use the `dummy_search_cmd` and `coord_gpu_cmd` printed
# PLEASE EDIT BEFORE RUN: retrieval_interval (EncDec): 8 / 64 / 512
python print_server_coordinator_gpu_cmd.py --model_base_config config/EncDec-L.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_EncDec-L_latency_only_inference.yaml

# Throughput: use the `dummy_search_cmd` and `coord_gpu_cmd` printed
# PLEASE EDIT BEFORE RUN: retrieval_interval (EncDec): 8 / 64 / 512
python print_server_coordinator_gpu_cmd.py --model_base_config config/EncDec-L.yaml --coordinator_base_config config/coordinator.yaml --runtime_config config/ralm_runtime_EncDec-L_throughput_only_inference.yaml
```